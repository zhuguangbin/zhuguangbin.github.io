
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Spark安装部署篇 - Data Talks</title>
  <meta name="author" content="Zhu Guangbin">

  
  <meta name="description" content="Spark有以下四种运行模式： local: 本地单进程模式，用于本地开发测试Spark代码
standalone：分布式集群模式，Master-Worker架构，Master负责调度，Worker负责具体Task的执行
on yarn/mesos: ‌运行在yarn/ &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://zhuguangbin.github.io/blog/2013/07/16/spark-deploy/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Data Talks" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:zhuguangbin.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
    <li><a href="/">
        <span class="blue_light">
            Data Talks
        </span>
       
           <span class="blue_dark">
             Play with Hadoop/Hive/HBase/Spark/Shark
           </span>
       
    </a></li>
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about">About me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Spark安装部署篇</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-07-16T11:58:00+08:00" pubdate data-updated="true">Jul 16<span>th</span>, 2013</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>  Spark有以下四种运行模式：</p>

<ul>
<li>local: 本地单进程模式，用于本地开发测试Spark代码</li>
<li>standalone：分布式集群模式，Master-Worker架构，Master负责调度，Worker负责具体Task的执行</li>
<li>on yarn/mesos: ‌运行在yarn/mesos等资源管理框架之上，yarn/mesos提供资源管理，spark提供计算调度，并可与其他计算框架(如MapReduce/MPI/Storm)共同运行在同一个集群之上</li>
<li>on cloud(EC2): 运行在AWS的EC2之上</li>
</ul>


<p>本章主要介绍本地模式和standalone模式的安装部署配置。</p>

<h3>本地模式</h3>

<p>  <em>注: 本文只介绍Linux的安装部署配置方式，Windows以及Mac下类似，用户请自行参考官方文档</em></p>

<h4>1. 前提条件</h4>

<p>  Spark依赖JDK 6.0以及Scala 2.9.3以上版本，所以首先确保已安装JDK和Scala的合适版本并加入PATH。
  本节比较简单，不在此赘述。安装完毕，请验证JDK和Scala的版本</p>

<pre><code>hadoop@Aspire-5830TG:~$ echo $JAVA_HOME
/usr/local/jdk
hadoop@Aspire-5830TG:~$ java -version
java version "1.6.0_43"
Java(TM) SE Runtime Environment (build 1.6.0_43-b01)
Java HotSpot(TM) 64-Bit Server VM (build 20.14-b01, mixed mode)

hadoop@Aspire-5830TG:~$ echo $SCALA_HOME
/usr/local/scala
hadoop@Aspire-5830TG:~$ scala
Welcome to Scala version 2.9.3 (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_43).
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; 
</code></pre>

<h4>2. 安装Spark</h4>

<p>  Spark的安装和简单，只需要将Spark的安装包download下来，加入PATH即可。我们采用官方v0.7.3版本</p>

<pre><code>cd /user/local/spark  
wget http://spark-project.org/download/spark-0.7.3-prebuilt-hadoop1.tgz 
tar zxvf spark-0.7.3-prebuilt-hadoop1.tgz  
ln -s spark-0.7.3 spark-release  

vim /etc/profile  
export SPARK_HOME=/usr/local/spark/spark-release
export PATH=$SPARK_HOME/bin
</code></pre>

<h4>3. 配置Spark</h4>

<p>  Spark的配置文件只有一个: $SPARK_HOME/conf/spark-env.sh。本地开发模式的配置很简单，只需要配置JAVA_HOME和SCALA_HOME。实例如下：</p>

<pre><code>export JAVA_HOME=/usr/local/jdk  
export SCALA_HOME=/usr/local/scala  
# add spark example jar to CLASSPATH  
export SPARK_EXAMPLES_JAR=$SPARK_HOME/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3.jar  
</code></pre>

<h4>4. 测试验证</h4>

<p>  Spark提供了两种运行模式：</p>

<p>  1) run脚本: 用于运行已经生成的jar包中的代码，如Spark自带的example中的SparkPi。</p>

<pre><code>hadoop@Aspire-5830TG:/usr/local/spark/spark-release$ ./run  spark.examples.SparkPi local

# 此处略去一万字....
Pi is roughly 3.1358
</code></pre>

<p>  2) spark-shell: 用于interactive programming</p>

<pre><code>hadoop@Aspire-5830TG:/usr/local/spark/spark-release$ ./spark-shell 
Welcome to
      ____              __  
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 0.7.3
      /_/                  

Using Scala version 2.9.3 (Java HotSpot(TM) 64-Bit Server VM, Java 1.6.0_43)
Initializing interpreter...
13/07/16 15:28:04 WARN Utils: Your hostname, Aspire-5830TG resolves to a loopback address: 127.0.0.1; using 172.16.239.1 instead (on interface vmnet8)
13/07/16 15:28:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Creating SparkContext...
Spark context available as sc.
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; val days = List("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
days: List[java.lang.String] = List(Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)

scala&gt; val daysRDD = sc.parallelize(days)
daysRDD: spark.RDD[java.lang.String] = ParallelCollectionRDD[0] at  parallelize at &lt;console&gt;:14

scala&gt; daysRDD.count()
res0: Long = 7

scala&gt; 
</code></pre>

<h3>Standalone模式</h3>

<p>  <em>Note：Spark是Master-Worker架构，在部署之前需要确定Master/Worker机器。同时，分布式集群模式要共享HDFS上的数据，因此需要在每个节点安装Hadoop。</em></p>

<h4>1. 前提条件</h4>

<ul>
<li>集群模式下，Master节点要能够<em>ssh无密码登陆</em>各个Worker节点。</li>
<li>与本地模式一样，Spark集群的<em>每个节点</em>需要安装JDK和Scala。</li>
<li>Spark集群的<em>每个节点</em>需要安装Hadoop（作为hadoop client访问HDFS上的数据）。</li>
</ul>


<h4>2. 安装Spark</h4>

<p>  与本地环境一样，Spark集群的<em>每个节点</em>需要download spark的tar.gz包，解压，并配置环境变量。</p>

<h4>3. 配置Spark</h4>

<p>  分布式集群模式下，Spark的配置文件有两个:</p>

<ul>
<li>$SPARK_HOME/conf/slaves</li>
</ul>


<p>  slaves 是一个文本文件，将各个worker节点的IP加进去，一行一个，示例如：</p>

<pre><code>10.2.6.133
10.2.6.134
10.2.6.154
</code></pre>

<ul>
<li>$SPARK_HOME/conf/spark-env.sh文件：</li>
</ul>


<p>  Spark的环境配置文件，定义Spark的Master/Worker以及资源定义，示例如：</p>

<pre><code>export JAVA_HOME=/usr/local/jdk 
export SCALA_HOME=/usr/local/scala

# SSH related
export SPARK_SSH_OPTS="-p58422 -o StrictHostKeyChecking=no"

# Spark Master IP
export SPARK_MASTER_IP=10.2.6.152 
# set the number of cores to use on worker
export SPARK_WORKER_CORES=16
# set how much memory to use on worker
export SPARK_WORKER_MEMORY=16g

export SPARK_EXAMPLES_JAR=/usr/local/spark/spark-0.7.3/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3.jar

# LZO codec related
export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib
export SPARK_LIBRARY_PATH=/usr/local/hadoop/hadoop-release/lib/native/Linux-amd64-64/  
</code></pre>

<p>  在Spark集群的<em>每个节点</em>上配置好以上两个配置文件</p>

<h4>4. 启动集群</h4>

<p>  在<em>Master</em>节点上，执行如下命令：</p>

<pre><code>[hadoop@cosmos152 spark-release]$ echo $SPARK_HOME
/usr/local/spark/spark-release
[hadoop@cosmos152 spark-release]$ bin/start-all.sh 
starting spark.deploy.master.Master, logging to /usr/local/spark/spark-0.7.3/bin/../logs/spark-hadoop-spark.deploy.master.Master-1-cosmos152.hadoop.out
Master IP: 10.2.6.152
cd /usr/local/spark/spark-0.7.3/bin/.. ; /usr/local/spark/spark-release/bin/start-slave.sh 1 spark://10.2.6.152:7077
10.2.6.133: starting spark.deploy.worker.Worker, logging to /usr/local/spark/spark-0.7.3/bin/../logs/spark-hadoop-spark.deploy.worker.Worker-1-cosmos133.hadoop.out
10.2.6.134: starting spark.deploy.worker.Worker, logging to /usr/local/spark/spark-0.7.3/bin/../logs/spark-hadoop-spark.deploy.worker.Worker-1-cosmos134.hadoop.out
10.2.6.154: starting spark.deploy.worker.Worker, logging to /usr/local/spark/spark-0.7.3/bin/../logs/spark-hadoop-spark.deploy.worker.Worker-1-cosmos154.hadoop.out
[hadoop@cosmos152 spark-release]$ 
</code></pre>

<h4>5. 测试验证</h4>

<p>  推荐部署一个Client节点上，专门用于Spark Job的提交。配置与上述节点一致。在Client节点上，执行以下命令：</p>

<pre><code>hadoop@cosmos155:/usr/local/spark/spark-release$ ./run  spark.examples.SparkPi spark://10.2.6.152:7077

# 此处略去一万字....
Pi is roughly 3.1358
</code></pre>

<h3>参考资料：</h3>

<ol>
<li><a href="http://spark-project.org/docs/latest/index.html">Spark Overview</a></li>
<li><a href="http://spark-project.org/docs/latest/spark-standalone.html">Standalone Deploy Mode</a></li>
</ol>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Zhu Guangbin</span></span>

      








  


<time datetime="2013-07-16T11:58:00+08:00" pubdate data-updated="true">Jul 16<span>th</span>, 2013</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://zhuguangbin.github.io/blog/2013/07/16/spark-deploy/" data-via="" data-counturl="http://zhuguangbin.github.io/blog/2013/07/16/spark-deploy/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/07/16/spark-core-concept/" title="Previous Post: Spark核心思想篇">&laquo; Spark核心思想篇</a>
      
      
        <a class="basic-alignment right" href="/blog/2013/07/16/spark-programming-user-guide-basic/" title="Next Post: Spark Programming User Guide Basic">Spark Programming User Guide Basic &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/07/23/shark-vs-hive-benchmark-test/">Shark vs Hive Benchmark Test</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/21/shark-deploy/">Shark安装部署与应用</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/16/spark-programming-examples/">Spark Programming Examples</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/16/spark-programming-user-guide-for-javauser/">Spark Programming User Guide For Java User</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/16/spark-programming-user-guide-for-scalauser/">Spark Programming User Guide For Scala User</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Zhu Guangbin -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
